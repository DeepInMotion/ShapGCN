import torch
from torch import nn
# adopted from
# https://github.com/yfsong0709/EfficientGCNv1


class HardSwish(nn.Module):
    def __init__(self, inplace=False):
        super(HardSwish, self).__init__()
        self.inplace = inplace

    def forward(self, x):
        inner = nn.functional.relu6(x + 3.).div_(6.)
        return x.mul_(inner) if self.inplace else x.mul(inner)


class AconC(nn.Module):
    """
    ACON activation (activate or not).
    AconC: (p1*x-p2*x) * sigmoid(beta*(p1*x-p2*x)) + p2*x, beta is a learnable parameter
    according to "Activate or Not: Learning Customized Activation" <https://arxiv.org/pdf/2009.04759.pdf>.
    """
    def __init__(self, channel):
        super(AconC, self).__init__()
        self.p1 = nn.Parameter(torch.randn(1, channel, 1, 1))
        self.p2 = nn.Parameter(torch.randn(1, channel, 1, 1))
        self.beta = nn.Parameter(torch.ones(1, channel, 1, 1))

    def forward(self, x):
        inner = (self.p1 * x - self.p2 * x) * (self.beta * (self.p1 * x - self.p2 * x)).sigmoid() + self.p2 * x
        return (self.p1 * x - self.p2 * x) * (self.beta * (self.p1 * x - self.p2 * x)).sigmoid() + self.p2 * x


class MetaAconC(nn.Module):
    """
    ACON activation (activate or not).
    MetaAconC: (p1*x-p2*x) * sigmoid(beta*(p1*x-p2*x)) + p2*x, beta is generated by a small network
    according to "Activate or Not: Learning Customized Activation" <https://arxiv.org/pdf/2009.04759.pdf>.
    """
    def __init__(self, channel, r=4):
        super(MetaAconC, self).__init__()
        inner_channel = max(r, channel // r)
        self.fcn = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channel, inner_channel, 1),
            nn.BatchNorm2d(inner_channel),
            nn.Conv2d(inner_channel, channel, 1),
            nn.BatchNorm2d(channel),
            nn.Sigmoid(),
        )
        self.p1 = nn.Parameter(torch.randn(1, channel, 1, 1))
        self.p2 = nn.Parameter(torch.randn(1, channel, 1, 1))

    def forward(self, x):
        return (self.p1 * x - self.p2 * x) * (self.fcn(x) * (self.p1 * x - self.p2 * x)).sigmoid() + self.p2 * x


class ReLU(nn.ReLU):
    def __init__(self, module_name, inplace=False):
        super(ReLU, self).__init__(inplace=inplace)
        self.module_name = module_name

    def __repr__(self):
        return f"ReLU_{self.module_name}"


class ReLU6(nn.ReLU6):
    def __init__(self, module_name, inplace=False):
        super(ReLU6, self).__init__(inplace=inplace)
        self.module_name = module_name

    def __repr__(self):
        return f"ReLU6_{self.module_name}"


class SiLU(nn.SiLU):
    def __init__(self, module_name, inplace=False):
        super(SiLU, self).__init__()
        self.module_name = module_name

    def __repr__(self):
        return f"SiLU_{self.module_name}"


class Hardswish(nn.Hardswish):
    def __init__(self, module_name, inplace=False):
        super(Hardswish, self).__init__()
        self.module_name = module_name

    def __repr__(self):
        return f"Hardswish_{self.module_name}"


class Sigmoid(nn.Sigmoid):
    def __init__(self, module_name):
        super(Sigmoid, self).__init__()
        self.module_name = module_name

    def __repr__(self):
        return f"Sigmoid_{self.module_name}"


class Tanh(nn.Tanh):
    def __init__(self, module_name):
        super(Tanh, self).__init__()
        self.module_name = module_name

    def __repr__(self):
        return f"Tanh_{self.module_name}"


class Swish(nn.Module):
    def __init__(self, module_name, inplace=False):
        super(Swish, self).__init__()
        self.inplace = inplace
        self.module_name = module_name

    def forward(self, x):
        return x * torch.sigmoid(x)

    def __repr__(self):
        return f"Swish_{self.module_name}"


class Activations:
    @staticmethod
    def get_activation(activation_str: str, module_name: str):
        module_name = module_name.split('(')[0].strip()
        if activation_str == "relu":
            return ReLU(module_name)
        elif activation_str == "relu6":
            return ReLU6(module_name)
        elif activation_str == "silu":
            return SiLU(module_name)
        elif activation_str == "hardswish":
            return Hardswish(module_name)
        elif activation_str == 'sigmoid':
            return Sigmoid(module_name)
        elif activation_str == 'tanh':
            return Tanh(module_name)
        elif activation_str == 'swish':
            return SiLU(module_name)
        elif activation_str == 'acon':
            return AconC(channel=8)
        elif activation_str == 'meta':
            return MetaAconC(channel=8)
        else:
            raise ValueError(f"Unsupported activation function: {activation_str}")
